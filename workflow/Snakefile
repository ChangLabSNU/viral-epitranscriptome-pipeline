import pandas as pd
import numpy as np
import subprocess as sp

SAMPLES_ALL = 'Korea_2020 Freiburg_2020_1 IVT_1 IVT_2'.split(' ')
SAMPLES_VIRAL = 'Korea_2020 Freiburg_2020_1'.split(' ')
SAMPLES_CTL = 'IVT_1 IVT_2'.split(' ')

ANNOTATION_VERSION = 'VerVet-SARSCoV2-DRS'
ANNOTATION_VERSION_HUMAN = 'human-SARSCoV2-DRS'
PICARD_CMD =  'java -jar /blaze/ari/picard/build/libs/picard.jar'

MINIMAP2_GENOME_INDEX = 'hy_/refs/{}.genome.mm2.idx'.format(ANNOTATION_VERSION)
MINIMAP2_GENOME_INDEX_HUMAN = 'refs/{}.genome.mm2.idx'.format(ANNOTATION_VERSION_HUMAN)
MINIMAP2_TRANSCRIPTOME_INDEX = 'hy_/refs/{}.transcriptome.mm2.idx'.format(ANNOTATION_VERSION)
MINIMAP2_VIRALGENOME_INDEX = 'hy_/refs/{}.viral_genome.mm2.idx'.format(ANNOTATION_VERSION)
JUNCTIONS = 'hy_/refs/SARS-CoV-2-introns-over100.bed'
VIRALGENOME = 'hy_/refs/SARS-CoV-2.fa'
VIRALTRANSCRIPTOME = 'hy_/refs/SARS-CoV-2-singlejumps.fa'
VIRUS_LEADER_RANGE = 'hy_/refs/SARS-CoV-2-leader.bed'


LANDING_POSITION_ANNOTATIONS = 'hy_/refs/SARS-CoV-2-landing-positions.bed'

SCV2_SUBGENOMIC_ORFS = sp.check_output(['cut', '-f4', LANDING_POSITION_ANNOTATIONS]).decode().split()
SCV2_ORFS = ['ORF1ab'] + SCV2_SUBGENOMIC_ORFS
FULL_LENGTH_PROBE = 'refs/SARS-CoV-2-fulllength-probe.bed'
FULL_LENGTH_LIMIT = 3000


rule all:
	input:
		expand('alignments/{name}.genome.sorted.bam', name=SAMPLES_ALL),
		expand('alignments/{name}.viral.sorted.bam', name=SAMPLES_ALL),	
		expand('alignments/{name}.viral_linear.sorted.bam', name=SAMPLES_ALL),
		expand('stats/{name}.viral_linear.cov.txt', name=SAMPLES_ALL),
		expand('junctions/{name}.canonical_swithing_annotations.txt.gz', name=SAMPLES_VIRAL),
		expand('junctions/{name}.subset-{orf}.readids', name=SAMPLES_VIRAL, orf=SCV2_ORFS),
		expand('alignments/{name}.balanced.sorted.bam', name=SAMPLES_VIRAL),	
		expand('sequences/{name}.balanced.fastq.gz', name=SAMPLES_VIRAL),
		expand('stats/{name}.balanced.cov.txt', name=SAMPLES_VIRAL),
		expand('stats/{name}.subset.cov.txt', name=SAMPLES_CTL),
		expand('eligos2/pairdiff/{name}_RCS/combine.txt', name=SAMPLES_VIRAL),
		expand('m6anet/{name}/2/data.result.csv.gz', name=SAMPLES_VIRAL')

rule merge_sequence:
	input: 'guppy_6.1.2/{name}/sequencing_summary.txt'
	output: 'sequences/{name}.pass.fastq.gz'
	threads: 4
	shell: 'zcat guppy_6.1.2/{wildcards.name}/pass/*.fastq.gz | bgzip -c /dev/stdin -@ {threads} > {output}'

rule map_reads_genome:
	input:
		seqs='sequences/{name}.pass.fastq.gz',
		
	output: 
		bam = 'alignments/{name,[^.]+}.genome.sorted.bam',
		idx = 'alignments/{name,[^.]+}.genome.sorted.bam.bai'
	threads: 20
	run:
		if ({wildcards.name} in SAMPLES_CTL) or ({wildcards.name} == 'Korea_2020'):
			idx = {MINIMAP2_GENOME_INDEX}
		else:
			idx = {MINIMAP2_GENOME_INDEX_HUMAN}
		shell('minimap2 --junc-bed {JUNCTIONS} \
		--MD -a -x splice -N 32 -un -t {threads} {idx} \
		{input.seqs} | samtools sort -o {output.bam}')

		shell('samtools index {output.bam}')

rule get_viral_reads:
	input:
		allreads='sequences/{name}.pass.fastq.gz',
		aln='alignments/{name}.genome.sorted.bam',
		bamix = 'alignments/{name}.genome.sorted.bam.bai'
	output:
		subsetfastq='sequences/{name}.viral.fastq.gz',
		readids = 'tmp/{name}.viral-reads'
	threads: 10
	run:
		shell('samtools view {input.aln} chrSCV | cut -f1 | sort | uniq > {output.readids}')
		shell('seqtk subseq {input.allreads} {output.readids} | bgzip -@ {threads} -c /dev/stdin > {output.subsetfastq}')

rule map_reads_viral_transcriptome:
	input:
		seqs='sequences/{name}.viral.fastq.gz',
		ref=VIRALGENOME
	output: bam = 'alignments/{name,[^.]+}.viral.sorted.bam',
		idx = 'alignments/{name}.viral.sorted.bam.bai'
	threads: 20
	run:
		shell('minimap2 -k 10 -w 3 --splice -g 30000 -G 30000 -A1 -B2 -O2,24 -E1,0 \
		-C0 -z 400,200 --no-end-flt -F 40000 -N 350 \
		--splice-flank=no --max-chain-skip=40 -un --MD -a -p 0.7 \
		--junc-bed={JUNCTIONS} --junc-bonus=50 \
		-t {threads} {input.ref} {input.seqs} | samtools sort  -o {output.bam}')
		
		shell('samtools index {output.bam}')

rule sort_chimeric_sequences:
	input:  bam = 'alignments/{name}.viral.sorted.bam',
		idx = 'alignments/{name}.viral.sorted.bam.bai'
	output: ids = 'tmp/{name}.viral.chimeric-ids',
		linearids = 'tmp/{name}.viral.linear-ids',
		linear = 'alignments/{name}.viral_linear.sorted.bam'
	run:
		shell('samtools view -F 2048 {input.bam} | cut -f1 | sort | uniq > {output.linearids}')
		shell('samtools view -f 2048 {input.bam} | cut -f1 | sort | uniq > {output.ids}')
		shell('{PICARD_CMD} FilterSamReads I={input.bam} O={output.linear} READ_LIST_FILE={output.linearids} FILTER=includeReadList')
		shell('samtools index {output.linear}')

rule coverage:
	input: 'alignments/{name}.viral_linear.sorted.bam'
	output: 'stats/{name}.viral_linear.cov.txt'
	shell: 'bedtools genomecov -ibam {input} -split -dz > {output}'


rule extract_junction:
	input: 'alignments/{name}.viral_linear.sorted.bam'
	output: 'junctions/{name}.large_deletions.bed.gz'
	shell: 'python ../scripts/extract-junctions.py {input} | \
            bgzip -c /dev/stdin > {output}'

rule annotate_jump_landings:
	input:  'junctions/{name}.large_deletions.bed.gz'
	output: 'junctions/{name}.canonical_swithing_annotations.txt.gz'
	shell: 'zcat {input} | \
            awk \'BEGIN {{ OFS="\t"; }} \
                {{ if ($2 >= 55 && $2 <= 85) {{ \
                    print $1, $3, $3+1, $4, $5, $6; \
                }}  }}\' | \
            sort -k4,4 -k2,2n | uniq -f3 | \
            bedtools intersect -bed -a - -b {LANDING_POSITION_ANNOTATIONS} -wa -wb | \
            cut -f4,10 | sed -e \'s,#[0-9]*,,g\' | sort -k1,1 | \
            bgzip -c /dev/stdin > {output}'

for orfname in SCV2_SUBGENOMIC_ORFS:
    rule: # split_readids_by_junction_match
        input: 'junctions/{name}.canonical_swithing_annotations.txt.gz'
        output: 'junctions/{{name}}.subset-{orf}.readids'.format(orf=orfname)
        params: orf=orfname
        run:
            matches = pd.read_csv(input[0], sep='\t', names=['read_id', 'orf'])
            subset = matches[matches['orf'] == params.orf]
            subset[['read_id']].drop_duplicates().to_csv(output[0],
                sep='\t', index=False, header=False)

rule get_readids_full_length:
    input: 'alignments/{name}.viral_linear.sorted.bam'
    output: 'junctions/{name}.subset-ORF1ab.readids'
    shell: 'bedtools intersect -abam {input} \
                -b {FULL_LENGTH_PROBE} -bed -wa -split | \
            awk \'{{ split($11, tok, ","); alnlen = 0; \
                     for(i in tok) alnlen += tok[i]; \
                     if (alnlen >= {FULL_LENGTH_LIMIT}) \
                        print $4; }}\' | \
            sort | uniq > {output}'

rule subset_alignments_by_trtype:
    input:
        alignment='alignments/{name}.viral_genome.linear.sorted.bam',
        readids='junctions/{name}.subset-{orf}.readids'
    output: 'subset-alignments/{name,[^.]+}.viral_genome.{orf}.bam'
    threads: 4
    shell: '{PICARD_CMD} FilterSamReads I={input.alignment} \
                O={output} READ_LIST_FILE={input.readids} \
                FILTER=includeReadList'

rule get_subset_ids:
	input: 'junctions/{name}.subset-N.readids'
	output: 'junctions/{name}.subset-all.readids'
	run:
		shell('touch {output}')
		for orf in SCV2_ORFS:
			if orf=='ORF1ab':
				shell('shuf -n 10000 junctions/{wildcards.name}.subset-{orf}.readids >> {output}')
			shell('shuf -n 4000 junctions/{wildcards.name}.subset-{orf}.readids >> {output}')

rule get_balanced_bamfile:
	input:	aln='alignments/{name}.viral_linear.sorted.bam',
		readids='junctions/{name}.subset-all.readids'
	output: 'alignments/{name}.balanced.sorted.bam'
	run: 
		shell('{PICARD_CMD} FilterSamReads I={input.aln} \
			O={output} READ_LIST_FILE={input.readids} \
                	FILTER=includeReadList')
		shell('samtools index {output}')

rule get_balanced_fastq:
	input: seqs='sequences/{name}.viral.fastq.gz',
		readids='junctions/{name}.subset-all.readids'
	output: 'sequences/{name}.balanced.fastq.gz'
	shell:'cat {input.readids}|sort|uniq|seqtk subseq {input.seqs} /dev/stdin | bgzip -@ {threads} -c /dev/stdin > {output}'


rule balanced_cov:
	input: 'alignments/{name}.balanced.sorted.bam'
	output: 'stats/{name}.balanced.cov.txt'
        shell: 'bedtools genomecov -ibam {input} -split -dz > {output}'

rule subset_IVT:
	input: aln='alignments/{name}.viral.sorted.bam',
		seqs='sequences/{name}.viral.fastq.gz',
	output: readids='junctions/{name}.subset.IVT.readids',
		aln='alignments/{name}.subset.sorted.bam',
		cov='stats/{name}.subset.cov.txt',
		seqs='sequences/{name}.subset.fastq.gz'
	run:
		shell('samtools view {input.aln} | cut -f1| sort | uniq | shuf -n 200000 > {output.readids}')
		shell('{PICARD_CMD} FilterSamReads I={input.aln} O={output.aln} READ_LIST_FILE={output.readids} FILTER=includeReadList')
		shell('samtools index {output.aln}')
		shell('seqtk subseq {input.seqs} {output.readids} | bgzip -c /dev/stdin > {output.seqs}')
		shell('bedtools genomecov -ibam {output.aln} -split -dz > {output.cov}')


rule perpare_IVTmerge:
	input: 'alignments/IVT_1.subset.sorted.bam', alignments/IVT_2.subset.sorted.bam'
	output: 
		merged = 'alignments/IVT.subset_merged.bam'
		sorted = 'alignments/IVT.subset_merged.sorted.bam'
	run:
		shell('{PICARD_CMD} GatherBamFiles I=input[0] I=input[1] O=output.merged')
		shell('samtools sort -o {output.sorted} {output.merged}'
		shell('samtools index {output.sorted}'


rule eligos2_pairdiff:
	input: 
		tbam = 'alignments/IVT.subset_merged.sorted.bam',
		cbam = 'alignments/{name}.balanced.sorted.bam',
		reg = 'refs/SARS-CoV-2-annotations.bed',
		ref = VIRALGENOME
	output: 'eligos2/pairdiff/{name}/combine.txt'
	params: outputdir = 'eligos2/pairdiff/{name}/'
	threads: 5
	run:
		ctl = 'IVT.subset_merged.sorted'
		outputpfx = f'{params.outputdir}/{wildcards.name}.balanced.sorted_vs_{ctl}_on_SARS-CoV2_'
		shell('conda run -n eligos2 ~/eligos2/eligos2 pair_diff_mod -tbam {input.tbam} -cbam {input.cbam} -reg {input.reg} -ref {input.ref} -t {threads} --pval 1 --oddR 0 --esb 0 -o {params.outputdir}')
		shell('ln {outputpfx}combine.txt {output}')
		shell('rm -r {parmas.outputdir}/tmp')

rule eligos2_pairdiff_RCS:
	input: cbam='alignments/IVT.merged.sorted.bam',
		tbam = 'alignments/{name}.genome.sorted.bam',
		reg= 'refs/ont-refs.bed',
		ref = 'chrRCS'
	output: 'eligos2/pairdiff/{name}_RCS/combine.txt'
	params: outputdir = 'eligos2/pairdiff/{name}_RCS'
	run:
		ctl = 'IVT.merged.sorted'
		outputpfx = f'{params.outputdir}/{wildcards.name}.genome.sorted_vs_{ctl}_on_ont-refs_'
		shell('conda run -n eligos2 ~/eligos2/eligos2 pair_diff_mod -tbam {input.tbam} -cbam {input.cbam} -reg {input.reg} -ref {input.ref} -t {threads} --pval 1 --oddR 0 --esb 0 -o {params.outputdir}')
                shell('ln {outputpfx}combine.txt {output}')
                shell('rm -r {parmas.outputdir}/tmp')


rule m6a_nanopolish:
	input: 
		aln='alignments/{name}.balanced.sorted.bam',
                seqs='sequences/{name}.balanced.fastq.gz'
	output: 
		eventalign = 'nanopolish/{name}/balanced_m6A_eventalign_reads.tsv',
                summary='nanopolish/{name}/balanced_m6A_summary.txt',
	threads: 5
	shell: 'conda run -n nanopolish nanopolish eventalign -t {threads} --signal-index -r {input.seq} -b {input.aln} -g {VIRALGENOME} --summary={output.summary} --scale-events -n > {output.eventalign}'

rule m6a_dataprep:
	input: 'nanopolish/{name}/balanced_m6A_eventalign_reads.tsv'
	output: 'm6anet/{name}/eventalign.index'
	params: 'm6anet/{name}/'
	shell: 'm6anet-dataprep -i {input.eventalign} -o {params}'


rule m6a_running:
	input: 'm6anet/{name}/eventalign.index'
	output: 'm6anet/{name}/2/data.result.csv.gz'
	params:
		dir1 = 'm6anet/{name}/',
		dir2 = 'm6anet/{name}/2/'
	threads: 3
	run:
		shell('mkdir {params.dir2}')
		shell('ln -sf {parmas.dir1}/* {params.dir2}')
		shell('m6anet-run_inference --input_dir m6anet/IVT_1/ --out_dir m6anet/{params.dir1} --n_process {threads}')
		shell('m6anet-run_inference --input_dir m6anet/IVT_2/ --out_dir m6anet/{params.dir2} --n_process {threads}')


